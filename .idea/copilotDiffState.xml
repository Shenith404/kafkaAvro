<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/KafkaAvro/KAFKA_COMMIT_BEHAVIOR.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/KafkaAvro/KAFKA_COMMIT_BEHAVIOR.md" />
              <option name="updatedContent" value="# Kafka Commit Behavior with AckMode.RECORD&#10;&#10;## When Using `AckMode.RECORD`:&#10;&#10;### Successful Processing Flow:&#10;```&#10;1. Message received from Kafka&#10;2. @KafkaListener method called (OrderConsumer.listen())&#10;3. Message processing starts&#10;4. Processing completes successfully&#10;5. ✅ METHOD RETURNS NORMALLY&#10;6.  SPRING KAFKA COMMITS OFFSET IMMEDIATELY&#10;7. Next message is processed&#10;```&#10;&#10;### Failed Processing Flow (Transient Error):&#10;```&#10;1. Message received from Kafka&#10;2. @KafkaListener method called&#10;3. Processing fails (RuntimeException thrown)&#10;4. ❌ METHOD THROWS EXCEPTION&#10;5. ⏸️ NO COMMIT HAPPENS&#10;6. DefaultErrorHandler catches exception&#10;7.  RETRY MECHANISM TRIGGERED&#10;8. Same message reprocessed (same offset)&#10;```&#10;&#10;### Failed Processing Flow (Permanent Error):&#10;```&#10;1. Message received from Kafka&#10;2. @KafkaListener method called&#10;3. Processing fails (IllegalArgumentException thrown)&#10;4. ❌ METHOD THROWS EXCEPTION&#10;5. ⏸️ NO COMMIT HAPPENS&#10;6. DefaultErrorHandler catches non-retryable exception&#10;7.  MESSAGE SENT TO DLQ&#10;8.  OFFSET COMMITTED (only after DLQ send succeeds)&#10;```&#10;&#10;## Key Points:&#10;&#10;###  **Commit Timing with AckMode.RECORD:**&#10;- **SUCCESS**: Commit happens **immediately** after method returns successfully&#10;- **FAILURE**: **No commit** until error is resolved (retry succeeds or DLQ succeeds)&#10;&#10;###  **Retry Behavior:**&#10;- Failed messages keep the **same offset**&#10;- Consumer **doesn't advance** until success or DLQ&#10;- Prevents message loss during temporary failures&#10;&#10;###  **Offset Management:**&#10;- `enable.auto.commit=false` in consumer config&#10;- Spring Kafka manages commits based on AckMode&#10;- Manual offset management ensures reliability&#10;&#10;### ⚡ **Performance Impact:**&#10;- More frequent commits (per record vs per batch)&#10;- Higher reliability but slightly lower throughput&#10;- Suitable for critical message processing&#10;&#10;## Configuration in Your Application:&#10;&#10;```java&#10;// In KafkaConfig.java&#10;containerProps.setAckMode(ContainerProperties.AckMode.RECORD);&#10;&#10;// In Consumer Config&#10;props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);&#10;```&#10;&#10;## Monitoring Commits:&#10;&#10;Check the logs for:&#10;- `=== COMMIT SUCCESSFUL ===` - Offset committed&#10;- `=== COMMIT FAILED ===` - Commit error&#10;- `&gt;&gt;&gt; COMMIT WILL HAPPEN NOW &lt;&lt;&lt;` - Success triggers commit&#10;- `&gt;&gt;&gt; NO COMMIT &lt;&lt;&lt;` - Failure prevents commit&#10;&#10;## Alternative AckModes:&#10;&#10;| AckMode | Commit Timing | Use Case |&#10;|---------|---------------|----------|&#10;| RECORD | After each successful message | High reliability |&#10;| BATCH | After all messages in batch | Better performance |&#10;| TIME | At regular intervals | Background processing |&#10;| COUNT | After N messages | Balanced approach |&#10;| MANUAL | Explicit acknowledgment | Full control |" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/KafkaAvro/src/main/java/com/ruhcom/KafkaAvro/kafka/CommitDemoConsumer.java">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/KafkaAvro/src/main/java/com/ruhcom/KafkaAvro/kafka/CommitDemoConsumer.java" />
              <option name="updatedContent" value="package com.ruhcom.KafkaAvro.kafka;&#10;&#10;import org.apache.kafka.clients.consumer.ConsumerRecord;&#10;import org.slf4j.Logger;&#10;import org.slf4j.LoggerFactory;&#10;import org.springframework.kafka.annotation.KafkaListener;&#10;import org.springframework.kafka.support.Acknowledgment;&#10;import org.springframework.kafka.support.KafkaHeaders;&#10;import org.springframework.messaging.handler.annotation.Header;&#10;import org.springframework.stereotype.Service;&#10;&#10;@Service&#10;public class CommitDemoConsumer {&#10;    private static final Logger log = LoggerFactory.getLogger(CommitDemoConsumer.class);&#10;&#10;    @KafkaListener(topics = &quot;${app.kafka.topic}&quot;, groupId = &quot;${spring.kafka.consumer.group-id}-commit-demo&quot;)&#10;    public void listenWithCommitDetails(&#10;            ConsumerRecord&lt;String, Object&gt; record,&#10;            @Header(KafkaHeaders.RECEIVED_TOPIC) String topic,&#10;            @Header(KafkaHeaders.RECEIVED_PARTITION_ID) int partition,&#10;            @Header(KafkaHeaders.OFFSET) long offset) {&#10;        &#10;        log.info(&quot;=== COMMIT DEMO CONSUMER ===&quot;);&#10;        log.info(&quot; Received message:&quot;);&#10;        log.info(&quot;   Topic: {}&quot;, topic);&#10;        log.info(&quot;   Partition: {}&quot;, partition);&#10;        log.info(&quot;   Offset: {}&quot;, offset);&#10;        log.info(&quot;   Key: {}&quot;, record.key());&#10;        log.info(&quot;   Value: {}&quot;, record.value());&#10;        &#10;        log.info(&quot; Processing message...&quot;);&#10;        &#10;        // Simulate processing time&#10;        try {&#10;            Thread.sleep(100);&#10;        } catch (InterruptedException e) {&#10;            Thread.currentThread().interrupt();&#10;        }&#10;        &#10;        log.info(&quot;✅ Processing completed successfully&quot;);&#10;        log.info(&quot; COMMIT WILL HAPPEN NOW for offset: {}&quot;, offset);&#10;        log.info(&quot;   Next message will start from offset: {}&quot;, offset + 1);&#10;        log.info(&quot;=============================&quot;);&#10;        &#10;        // When this method returns, Spring Kafka will:&#10;        // 1. Call consumer.commitSync() for this specific record's offset&#10;        // 2. Update the committed offset to (current_offset + 1)&#10;        // 3. If commit fails, it will retry according to commit retry configuration&#10;    }&#10;}" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/KafkaAvro/src/main/resources/application.yml">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/KafkaAvro/src/main/resources/application.yml" />
              <option name="originalContent" value="server:&#10;  port: ${SERVER_PORT:8080}&#10;&#10;logging:&#10;  level:&#10;    com.ruhcom.KafkaAvro: DEBUG&#10;    org.springframework.kafka: INFO&#10;    org.apache.kafka: WARN&#10;  pattern:&#10;    console: &quot;%d{yyyy-MM-dd HH:mm:ss} - %msg%n&quot;&#10;&#10;app:&#10;  kafka:&#10;    topic: order&#10;    dlq-topic: order-dlq&#10;    replication-factor: ${KAFKA_REPLICATION_FACTOR:3}&#10;    partitions: ${KAFKA_PARTITIONS:5}&#10;    retry-attempts: ${KAFKA_RETRY_ATTEMPTS:5}&#10;    retry-backoff-ms: ${KAFKA_RETRY_BACKOFF_MS:3000}&#10;    consumer-concurrency: ${KAFKA_CONSUMER_CONCURRENCY:3}&#10;&#10;spring:&#10;  kafka:&#10;    bootstrap-servers: ${SPRING_KAFKA_BOOTSTRAP_SERVERS}&#10;    properties:&#10;      schema-registry-url: ${SPRING_KAFKA_SCHEMA_REGISTRY_URL}&#10;&#10;    producer:&#10;      key-serializer: org.apache.kafka.common.serialization.StringSerializer&#10;      value-serializer: io.confluent.kafka.serializers.KafkaAvroSerializer&#10;      client-id: order-service-producer&#10;&#10;    consumer:&#10;      group-id: ${SPRING_KAFKA_CONSUMER_GROUP_ID}&#10;      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer&#10;      value-deserializer: io.confluent.kafka.serializers.KafkaAvroDeserializer&#10;      auto-offset-reset: earliest&#10;      properties:&#10;        specific.avro.reader: true&#10;&#10;" />
              <option name="updatedContent" value="server:&#10;  port: ${SERVER_PORT:8080}&#10;&#10;logging:&#10;  level:&#10;    com.ruhcom.KafkaAvro: DEBUG&#10;    org.springframework.kafka: INFO&#10;    org.apache.kafka: WARN&#10;  pattern:&#10;    console: &quot;%d{yyyy-MM-dd HH:mm:ss} - %msg%n&quot;&#10;&#10;app:&#10;  kafka:&#10;    topic: order&#10;    dlq-topic: order-dlq&#10;    replication-factor: ${KAFKA_REPLICATION_FACTOR:3}&#10;    partitions: ${KAFKA_PARTITIONS:5}&#10;    retry-attempts: ${KAFKA_RETRY_ATTEMPTS:5}&#10;    retry-backoff-ms: ${KAFKA_RETRY_BACKOFF_MS:3000}&#10;    consumer-concurrency: ${KAFKA_CONSUMER_CONCURRENCY:3}&#10;&#10;spring:&#10;  kafka:&#10;    bootstrap-servers: ${SPRING_KAFKA_BOOTSTRAP_SERVERS}&#10;    properties:&#10;      schema-registry-url: ${SPRING_KAFKA_SCHEMA_REGISTRY_URL}&#10;&#10;    producer:&#10;      key-serializer: org.apache.kafka.common.serialization.StringSerializer&#10;      value-serializer: io.confluent.kafka.serializers.KafkaAvroSerializer&#10;      client-id: order-service-producer&#10;&#10;    consumer:&#10;      group-id: ${SPRING_KAFKA_CONSUMER_GROUP_ID}&#10;      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer&#10;      value-deserializer: io.confluent.kafka.serializers.KafkaAvroDeserializer&#10;      auto-offset-reset: earliest&#10;      properties:&#10;        specific.avro.reader: true&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>